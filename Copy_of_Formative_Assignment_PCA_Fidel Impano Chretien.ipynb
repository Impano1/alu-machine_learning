{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Impano1/alu-machine_learning/blob/main/Copy_of_Formative_Assignment_PCA_Fidel%20Impano%20Chretien.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:300/1*mgncZaKaVx9U6OCQu_m8Bg.jpeg\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "The goal of PCA is to extract information while reducing the number of features\n",
        "from a dataset by identifying which existing features relate to another. The crux of the algorithm is trying to determine the relationship between existing features, called principal components, and then quantifying how relevant these principal components are. The principal components are used to transform the high dimensional data to a lower dimensional data while preserving as much information. For a principal component to be relevant, it needs to capture information about the features. We can determine the relationships between features using covariance."
      ],
      "metadata": {
        "id": "xyATLU4z1cYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary package\n",
        "#TO DO\n",
        "import numpy as np\n",
        "import numpy.linalg as eig"
      ],
      "metadata": {
        "id": "UTntK0eUNimH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = np.array([\n",
        "    [   1,   2,  -1,   4,  10],\n",
        "    [   3,  -3,  -3,  12, -15],\n",
        "    [   2,   1,  -2,   4,   5],\n",
        "    [   5,   1,  -5,  10,   5],\n",
        "    [   2,   3,  -3,   5,  12],\n",
        "    [   4,   0,  -3,  16,   2],\n",
        "])"
      ],
      "metadata": {
        "id": "qWaiAdz8PyKp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Standardize the Data along the Features\n",
        "\n",
        "![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxe5VYCBsaZddkkTZlCY24Yov4JJD4-ArTA&usqp=CAU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explain why we need to handle the data on the same scale.\n",
        "\n",
        "**[TO DO: Insert Answer here]**"
      ],
      "metadata": {
        "id": "U2U2_Q5ebos3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "means = np.mean(data, axis=0)\n",
        "\n",
        "stds = np.std(data, axis=0)\n",
        "\n",
        "standardized_data = (data - means) / stds\n",
        "\n",
        "# the standardized data\n",
        "print(standardized_data)"
      ],
      "metadata": {
        "id": "JF3eGB7FRC0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86cff212-58a7-4197-ace2-4cb33c26266b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.36438208  0.70710678  1.5109662  -0.99186978  0.77802924]\n",
            " [ 0.12403473 -1.94454365 -0.13736056  0.77145428 -2.06841919]\n",
            " [-0.62017367  0.1767767   0.68680282 -0.99186978  0.20873955]\n",
            " [ 1.61245155  0.1767767  -1.78568733  0.33062326  0.20873955]\n",
            " [-0.62017367  1.23743687 -0.13736056 -0.77145428  1.00574511]\n",
            " [ 0.86824314 -0.35355339 -0.13736056  1.65311631 -0.13283426]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cov matrix.webp](https://dmitry.ai/uploads/default/original/1X/9bd2851674ebb55e404cc3ff5e2ffe65b42ff460.png)\n",
        "\n",
        "We use the pair - wise covariance of the different features to determine how they relate to each other. With these covariances, our goal is to group / cluster based on similar patterns. Intuitively, we can relate features if they have similar covariances with other features."
      ],
      "metadata": {
        "id": "7rzoiQ7fMk_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Calculate the Covariance Matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "uuhux3UEcBgw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn8oujZlK9YR",
        "outputId": "29746926-21d4-4ce8-83d7-dbd8907ba998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2.16666667  -1.06666667  -1.76666667   5.5         -4.36666667]\n",
            " [ -1.06666667   4.26666667   0.46666667  -6.6         19.66666667]\n",
            " [ -1.76666667   0.46666667   1.76666667  -3.3          2.36666667]\n",
            " [  5.5         -6.6         -3.3         24.7        -27.9       ]\n",
            " [ -4.36666667  19.66666667   2.36666667 -27.9         92.56666667]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "cov_matrix = np.cov(data.T)\n",
        "print(cov_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Eigendecomposition on the Covariance Matrix\n"
      ],
      "metadata": {
        "id": "uXNcG4AFcT08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "print(\"eigenvalues: \", eigenvalues)\n",
        "print(\"eigenvectors: \", eigenvectors)"
      ],
      "metadata": {
        "id": "dmGlQ47tRO5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc550fe-0e2e-4ee2-82fc-a4645e334678"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eigenvalues:  [1.07224751e+02 1.61823788e+01 1.93173735e+00 1.27579741e-01\n",
            " 2.20003762e-04]\n",
            "eigenvectors:  [[-0.05817655 -0.2631212   0.57237125  0.6292347  -0.45148374]\n",
            " [ 0.19774895 -0.03283879  0.06849106 -0.60720902 -0.7657827 ]\n",
            " [ 0.0328828   0.17887983 -0.75671562  0.45776292 -0.42983171]\n",
            " [-0.33200499 -0.88598416 -0.30234056 -0.11461168  0.01609676]\n",
            " [ 0.91989252 -0.33574235 -0.06059523  0.11259736  0.15724145]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Sort the Principal Components\n",
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list"
      ],
      "metadata": {
        "id": "4pWho88fcbJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "order_of_importance = np.argsort(eigenvalues)[::-1]\n",
        "print ( 'the order of importance is :\\n {}'.format(order_of_importance))\n",
        "\n",
        "# utilize the sort order to sort eigenvalues and eigenvectors\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]\n",
        "\n",
        "print('\\n\\n sorted eigen values:\\n{}'.format(sorted_eigenvalues))\n",
        "sorted_eigenvectors = eigenvectors[:, order_of_importance]\n",
        "print('\\n\\n The sorted eigen vector matrix is: \\n {}'.format(sorted_eigenvectors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znKtzdrTmMg",
        "outputId": "dd46131c-c219-4f1b-dd8f-78ac8d325d69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the order of importance is :\n",
            " [0 1 2 3 4]\n",
            "\n",
            "\n",
            " sorted eigen values:\n",
            "[1.07224751e+02 1.61823788e+01 1.93173735e+00 1.27579741e-01\n",
            " 2.20003762e-04]\n",
            "\n",
            "\n",
            " The sorted eigen vector matrix is: \n",
            " [[-0.05817655 -0.2631212   0.57237125  0.6292347  -0.45148374]\n",
            " [ 0.19774895 -0.03283879  0.06849106 -0.60720902 -0.7657827 ]\n",
            " [ 0.0328828   0.17887983 -0.75671562  0.45776292 -0.42983171]\n",
            " [-0.33200499 -0.88598416 -0.30234056 -0.11461168  0.01609676]\n",
            " [ 0.91989252 -0.33574235 -0.06059523  0.11259736  0.15724145]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "1. Why do we order eigen values and eigen vectors?\n",
        "\n",
        "[Insert Answer here]\n",
        "\n",
        "2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer\n",
        "\n",
        "[insert answer here]\n"
      ],
      "metadata": {
        "id": "o1nILNGxpTJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You want to see what percentage of information each eigen value holds. You would have print out the percentage of each eigen value using the formula\n",
        "\n",
        "\n",
        "\n",
        "> (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "\n"
      ],
      "metadata": {
        "id": "BWqFGNeNvgEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum_of_eigenvalues = np.sum(sorted_eigenvalues)\n",
        "\n",
        "print(sum_of_eigenvalues)\n",
        "\n",
        "percentages = (sorted_eigenvalues / sum_of_eigenvalues)\n",
        "\n",
        "print(percentages)\n",
        "\n",
        "for indexe, value in enumerate(sorted_eigenvalues):\n",
        "  print(f'Eigenvalue {indexe + 1}: {value: .8f}, with the percentage of: ({percentages[indexe]:.2f}%)')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMHrffrVOXR",
        "outputId": "a6c7d822-56de-4054-81d7-3f2f3303efe8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125.46666666666664\n",
            "[8.54607471e-01 1.28977515e-01 1.53964188e-02 1.01684172e-03\n",
            " 1.75348375e-06]\n",
            "Eigenvalue 1:  107.22475075, with the percentage of: (0.85%)\n",
            "Eigenvalue 2:  16.18237882, with the percentage of: (0.13%)\n",
            "Eigenvalue 3:  1.93173735, with the percentage of: (0.02%)\n",
            "Eigenvalue 4:  0.12757974, with the percentage of: (0.00%)\n",
            "Eigenvalue 5:  0.00022000, with the percentage of: (0.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize the number of Principle components then perfrom matrix multiplication with the variable K example k = 3 for 3 priciple components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> The reulting matrix (with reduced data) = standardized data * vector with columns k\n",
        "\n",
        "See expected output for k = 2\n",
        "\n"
      ],
      "metadata": {
        "id": "qB7H4InbfKx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 2  # select the number of principal components\n",
        "\n",
        "reduced_data = sorted_eigenvectors[:, order_of_importance[:k]]\n",
        "reduced_data = np.matmul(standardized_data, reduced_data) # transform the original data"
      ],
      "metadata": {
        "id": "C-Rnyq6QVTiz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxBcgQMXe1h",
        "outputId": "b8673e5f-08ee-49fd-b1aa-0f74079f3a84"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.31389845  1.22362226]\n",
            " [-2.55511419  0.01760889]\n",
            " [ 0.61494463  1.08892909]\n",
            " [-0.03531847 -1.11250845]\n",
            " [ 1.45756867  0.44379893]\n",
            " [-0.7959791  -1.66145072]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEqS6cuaMSY",
        "outputId": "d8dc7dfe-4203-4f32-c74e-94c7d0b1b808"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *What are 2 positive effects and 2 negative effects of PCA\n",
        "\n",
        "Give 2 Benefits and 2 limitations\n",
        "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. Here are two positive effects and two negative effects of PCA:\n",
        "\n",
        "Positive Effects of PCA\n",
        "Enhanced Visualization: PCA facilitates the reduction of high-dimensional data into two or three dimensions, making it easier to visualize and interpret complex datasets. This can reveal underlying patterns and structures that are not easily discernible in higher dimensions.\n",
        "\n",
        "Improved Computational Efficiency: By reducing the number of features, PCA can significantly decrease the computational load required for processing data. This can lead to faster training times for machine learning models and more efficient data storage and handling.\n",
        "\n",
        "Negative Effects of PCA\n",
        "Potential Loss of Important Information: During the dimensionality reduction process, some of the original data's variance is inevitably discarded. This can lead to the loss of critical information, especially if the removed dimensions contain subtle but significant patterns.\n",
        "\n",
        "Complexity in Component Interpretation: The principal components are often linear combinations of the original variables, which can be difficult to interpret. This can make it challenging to understand the contribution of each original variable to the principal components and the overall results.\n",
        "\n",
        "Benefits of PCA\n",
        "Enhanced Model Performance: Reducing the number of features helps mitigate the curse of dimensionality, potentially leading to improved performance and generalization of machine learning models. This is particularly beneficial for algorithms that are sensitive to the number of input features.\n",
        "\n",
        "Noise Reduction: By focusing on the components that capture the most variance, PCA can help eliminate noise and irrelevant features from the data. This results in cleaner datasets that can enhance the accuracy of subsequent analyses and models.\n",
        "\n",
        "Limitations of PCA\n",
        "Linearity Assumption: PCA assumes that the principal components are linear combinations of the original features. This assumption may not hold true for datasets with complex, non-linear relationships, potentially leading to suboptimal dimensionality reduction.\n",
        "\n",
        "Scaling Sensitivity: PCA is sensitive to the scale of the input data. Variables with larger scales can disproportionately influence the principal components. Therefore, proper standardization of the data is essential before applying PCA to ensure balanced contribution from all features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UxQ8lTunauMQ"
      }
    }
  ]
}